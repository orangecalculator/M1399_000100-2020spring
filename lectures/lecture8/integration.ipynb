{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessionInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(statmod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical integration\n",
    "\n",
    "### Goal\n",
    "\n",
    "To evaluate $\\int_a^b f(x) dx$, $f:[a, b] \\to \\mathbb{R}$.\n",
    "\n",
    "* Only a few functions can be integrated analytically\n",
    "* For the rest, we resort to numerical approximation\n",
    "\n",
    "Why do we need integrals?\n",
    "\n",
    "1. Expectation\n",
    "    - $X$ has some density on $[0, 1]$; $Y = \\sin(X)$\n",
    "    - $\\mathbf{E}[Z] = \\int_0^1 \\sin(x)f_X(x)dx$\n",
    "    \n",
    "2. Bayesian inference\n",
    "    - Parameter $\\theta$ has a prior density $f_{\\Theta}(\\theta)$ on $\\mathbb{R}$\n",
    "    - Likelihood of data $x$ is $f_{X|\\Theta}(x|\\theta)$\n",
    "    - Want to evaluate the posterior density\n",
    "$$\n",
    "    f_{\\Theta|X}(\\theta|x) = \\frac{f_{X|\\Theta}(x|\\theta)f_{\\Theta}(\\theta)}{\\int_{-\\infty}^{\\infty} f_{X|\\Theta}(x|\\theta')f_{\\Theta}(\\theta')d\\theta'}\n",
    "$$\n",
    "    - The integral in the denominator may not be analytically solved.\n",
    "\n",
    "3. MLE of structured data\n",
    "    - Data: $y_{ij}$ = # recalled words by $i$th patient of Alzheimer's disease in $j$th month, $i=1,\\dotsc, n$, $j=1, \\dotsc, T$\n",
    "    - Model:\n",
    "\\begin{align*}\n",
    "    Y_{ij} | \\lambda_{ij} &\\stackrel{indep}{\\sim} \\text{Poi}(\\lambda_{ij}), \\quad j=1,\\dotsc, T\\\\\n",
    "    \\lambda_{ij} &= \\exp(\\gamma_i + \\beta_0 + j\\beta_1) \\\\\n",
    "    \\gamma_i &\\stackrel{i.i.d.}{\\sim} N(0, \\sigma^2), \\quad i=1, \\dots, n\n",
    "\\end{align*}\n",
    "    - Called a generaized linear mixed model (here, Poisson GLM with a *random intercept*)\n",
    "    - Likelihood:\n",
    "\\begin{align*}\n",
    "    L(\\beta_0, \\beta_1, \\sigma^2) &= \\prod_{i=1}^n p(y_{i1}, \\dotsc, y_{iT}) \\\\\n",
    "        &= \\prod_{i=1}^n \\int_{-\\infty}^{\\infty} \\prod_{j=1}^T p(y_{ij} | \\lambda_{ij}(\\gamma_i) ) f(\\gamma_i)d\\gamma_i \\\\\n",
    "        &= \\prod_{i=1}^n \\int_{-\\infty}^{\\infty} \\prod_{j=1}^T e^{-\\lambda_{ij}(\\gamma_i)}\\frac{\\lambda_{ij}^{y_{ij}}(\\gamma_i)}{y_{ij}!}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{\\gamma_i^2}{2\\sigma^2}}d\\gamma_i, \\\\\n",
    "    \\text{where}~~ & \\lambda_{ij}(\\gamma_i) = \\exp(\\gamma_i + \\beta_0 + j\\beta_1).    \n",
    "\\end{align*}\n",
    "    - Likelihood function (hence score function) consists of integrals, which in most case do not have closed form.\n",
    "    \n",
    "What should we do?\n",
    "\n",
    "* Quadrature methods (now) - goes back to Archimedes\n",
    "* Monte Carlo methods (next) - 20th century invention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton-C么tes Quadrature\n",
    "\n",
    "* Key idea: \n",
    "    1. Subdivide the integration domain $[a, b]$ into $n$ *equally spaced* subintervals $[x_i, x_{i+1}]$ of length $h=\\frac{b - a}{n}$, i.e., $x_i = a + i h$, $i=0, 1, \\dotsc, n-1$.\n",
    "    2. Approximate the integrand $f$ on $[x_i, x_{i+1}]$ by an *interpolating polynomial* $p_i$ of order $m$.\n",
    "    3. Approximate $I_i = \\int_{x_i}^{x_{i+1}}f(x)dx$ by $\\int_{x_i}^{x_{i+1}}p_i(x)dx$. We know compute to do the latter exactly.\n",
    "    4. Approximate the integral $I=\\int_a^b f(x)dx$ by $\\sum_{i=0}^{n-1} I_i$.\n",
    "    \n",
    "* If $n$ is sufficiently large, the approximation will be accurate enough.    \n",
    "\n",
    "* Choice of order $m$ makes difference:\n",
    "    - $m = 0$: Riemann (rectangular) rule\n",
    "    - $m = 1$: Trapezoidal rule\n",
    "    - $m = 2$: Simpson's rule\n",
    "\n",
    "<img src=\"./quadrature.png\" width=\"600\" align=\"center\"/>\n",
    "(Taken from Givens and Hoeting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Riemann rule\n",
    "\n",
    "* Approximate $f$ on $[x_i, x_{i+1}]$ by a constant function $f(x_i)$:\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx \\approx \\int_{x_i}^{x_{i+1}}f(x_i)dx = (x_{i+1} - x_i)f(x_i) = h f(x_i).\n",
    "$$\n",
    "\n",
    "* How accurate is the Riemann rule? Assume that $f$ is four-times continuously differentiable. Then,\n",
    "$$\n",
    "    f(x) = f(x_i) + (x - x_i)f'(x_i) + \\frac{1}{2}(x - x_i)^2 f''(x_i) + \\frac{1}{6}(x - x_i)^3f^{(3)}(x_i) + O(|x - x_i|^4)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx = h f(x_i) + \\frac{h^2}{2}f'(x_i) + \\frac{h^3}{6}f''(x_i) + O(h^4)\n",
    "    .\n",
    "$$\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "    \\text{(error)} = \\sum_{i=0}^{n-1}(I_i - h f(x_i)) &= \\frac{h^2}{2}\\sum_{i=0}^{n-1}f'(x_i) + n O(h^3) \\\\\n",
    "    &\\le \\frac{(b-a)^2}{2n^2} n \\sup_{x\\in [a, b]}|f'(x)| + n O\\left(\\frac{(b-a)^3}{n^3}\\right) \\\\\n",
    "    &= O(n^{-1})\n",
    "    .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riemann <- function(f, a, b, n) {\n",
    "    h <- (b - a) / n   \n",
    "    \n",
    "    xi <- seq.int(a, b, length.out = n + 1)\n",
    "    xi <- xi[-1]\n",
    "    xi <- xi[-length(xi)]\n",
    "\n",
    "    intgrl <- h * (f(a) + sum(f(xi)) + f(b))\n",
    "    \n",
    "    return(intgrl)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f <- function(x) sin(x)  # integrand: example above with uniform X\n",
    "(truth <- 1 - cos(1)) # true integral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "riemann(f, 0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsub <- c(10, 100, 1000, 10000, 100000)\n",
    "err_R <- numeric(length(numsub))\n",
    "for (i in seq_along(numsub)) {\n",
    "    n <- numsub[i]\n",
    "    R <- riemann(f, 0, 1, n)\n",
    "    err_R[i] <- R - truth\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=numsub, y=abs(err_R))) + geom_line(aes(x=numsub, y=abs(err_R))) + \n",
    "scale_x_log10() + scale_y_log10() + theme_bw(base_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trapezoidal rule\n",
    "\n",
    "* Approximate $f$ on $[x_i, x_{i+1}]$ by a linear function interpolating $(x_i, f(x_i))$ and $(x_{i+1}, f(x_{i+1})$, i.e.,\n",
    "$$\n",
    "    p_i(x) = f(x_i) + \\frac{f(x_{i+1}) - f(x_i)}{x_{i+1} - x_i}(x - x_i)\n",
    "    .\n",
    "$$\n",
    "So\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx \\approx \\int_{x_i}^{x_{i+1}}p_i(x)dx = \\frac{x_{i+1} - x_i}{2}[f(x_i) + f(x_{i+1})] = \\frac{h}{2} [f(x_i) + f(x_{i+1})].\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    I = \\int_a^b f(x)dx \\approx h\\left(\\frac{1}{2}f(a) + f(x_1) + \\dotsb + f(x_{n-1}) + \\frac{1}{2}f(b)\\right) =: T(n)\n",
    "    .\n",
    "$$\n",
    "* How accurate is the Trapezoidal rule? Assume again that $f$ is four-times continuously differentiable. Then,\n",
    "$$\n",
    "    f(x) = f(x_i) + (x - x_i)f'(x_i) + \\frac{1}{2}(x - x_i)^2 f''(x_i) + \\frac{1}{6}(x - x_i)^3f^{(3)}(x_i) + O(|x - x_i|^4)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx = h f(x_i) + \\frac{h^2}{2}f'(x_i) + \\frac{h^3}{6}f''(x_i) + \\frac{h^4}{24}f^{(3)}(x_i) + O(h^5)\n",
    "    .\n",
    "$$\n",
    "Also,\n",
    "\\begin{align*}\n",
    "    \\int_{x_i}^{x_{i+1}}p_i(x)dx &= \\frac{h}{2} [f(x_i) + f(x_i+h)] \\\\\n",
    "    &= \\frac{h}{2}\\left( f(x_i) + f(x_i) + h f'(x_i) + \\frac{h^2}{2}f''(x_i) + \\frac{h^3}{6}f^{(3)}(x_i) + O(h^4) \\right) \\\\\n",
    "    &= h f(x_i) + \\frac{h^2}{2}f'(x_i) + \\frac{h^3}{4}f''(x_i) + \\frac{h^4}{12}f^{(3)}(x_i) + O(h^5)\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "    T(n) - I = \\sum_{i=0}^{n-1}\\left(I_i - \\int_{x_i}^{x_{i+1}}p_i(x)dx\\right) &= \\frac{h^3}{12}\\sum_{i=0}^{n-1}f''(x_i) + n O(h^4) \\\\\n",
    "    &\\le \\frac{(b-a)^3}{12n^3} n \\sup_{x\\in [a, b]}|f''(x)| + n O\\left(\\frac{(b-a)^4}{n^4}\\right) \\\\\n",
    "    &= O(n^{-2})\n",
    "    .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trapezoidal <- function(f, a, b, n) {\n",
    "    h <- (b - a) / n   \n",
    "    \n",
    "    xi <- seq.int(a, b, length.out = n + 1)\n",
    "    xi <- xi[-1]\n",
    "    xi <- xi[-length(xi)]\n",
    "\n",
    "    intgrl <- h * (0.5 * f(a) + sum(f(xi)) + 0.5 * f(b))\n",
    "    \n",
    "    return(intgrl)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f <- function(x) sqrt(1-x^2)   # integrand\n",
    "trapezoidal(f, 0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsub <- c(10, 100, 1000, 10000, 100000)\n",
    "err_T <- numeric(length(numsub))\n",
    "for (i in seq_along(numsub)) {\n",
    "    n <- numsub[i]\n",
    "    T <- trapezoidal(f, 0, 1, n)\n",
    "    err_T[i] <- T - truth\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=numsub, y=abs(err_T))) + geom_line(aes(x=numsub, y=abs(err_T))) + \n",
    "scale_x_log10() + scale_y_log10() + theme_bw(base_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simpson's rule\n",
    "\n",
    "* Approximate $f$ on $[x_i, x_{i+1}]$ by a quadratic function interpolating $(x_i, f(x_i))$, $\\left(\\frac{x_i + x_{i+1}}{2}, f\\left(\\frac{x_i + x_{i+1}}{2}\\right)\\right)$, and $(x_{i+1}, f(x_{i+1})$, i.e., *equal-spacing* of interpolation points within the equally spaced intervals $[x_i, x_{i+1}]$. The interpolating quadratic is\n",
    "\\begin{align*}\n",
    "    p_i(x) &= f(x_i)\\frac{x - \\frac{x_i + x_{i+1}}{2}}{x_i - \\frac{x_i + x_{i+1}}{2}}\\frac{x - x_{i+1}}{x_i - x_{i+1}} \\\\\n",
    "    & + f\\left(\\frac{x_i + x_{i+1}}{2}\\right)\\frac{x - x_i}{\\frac{x_i + x_{i+1}}{2} - x_i}\\frac{x - x_{i+1}}{\\frac{x_i + x_{i+1}}{2} - x_{i+1}} \\\\\n",
    "    & + f(x_i)\\frac{x - x_i}{x_{i+1} - x_i}\\frac{x - \\frac{x_i + x_{i+1}}{2}}{x_{i+1} - \\frac{x_i + x_{i+1}}{2}}\n",
    "    .\n",
    "\\end{align*}\n",
    "So\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx \\approx \\int_{x_i}^{x_{i+1}}p_i(x)dx = \\frac{h}{6} \\left[f(x_i) + 4f\\left(\\frac{x_i + x_{i+1}}{2}\\right) + f(x_{i+1})\\right].\n",
    "$$\n",
    "and\n",
    "\\begin{align*}\n",
    "    I &= \\int_a^b f(x)dx \\approx h\\left(\\frac{1}{6}f(a) + \\frac{2}{3}f(\\frac{x_0 + x_1}{2}) + \\frac{1}{3}f(x_1) + \\frac{2}{3}f(\\frac{x_1 + x_2}{2}) + \\dotsb + \\frac{1}{3}f(x_{n-1}) + \\frac{2}{3}f(\\frac{x_{n-1} + x_n}{2}) + \\frac{1}{6}f(b)\\right) \\\\\n",
    "    &=: S(n)\n",
    "    .\n",
    "\\end{align*}\n",
    "* How accurate is the Simpson rule? Assume  that $f$ is five-times continuously differentiable. Then,\n",
    "$$\n",
    "    f(x) = f(x_i) + (x - x_i)f'(x_i) + \\frac{1}{2}(x - x_i)^2 f''(x_i) + \\frac{1}{6}(x - x_i)^3f^{(3)}(x_i) + \\frac{1}{24}(x - x_i)^4f^{(4)}(x_i) + O(|x - x_i|^5)\n",
    "$$\n",
    "and\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx = h f(x_i) + \\frac{h^2}{2}f'(x_i) + \\frac{h^3}{6}f''(x_i) + \\frac{h^4}{24}f^{(3)}(x_i) + \\frac{h^5}{120}f^{(4)}(x_i) + O(h^6)\n",
    "    .\n",
    "$$\n",
    "Also,\n",
    "\\begin{align*}\n",
    "    \\int_{x_i}^{x_{i+1}}p_i(x)dx &= \\frac{h}{6} \\left[f(x_i) + 4f\\left(x_i + \\frac{h}{2}\\right) + f(x_i + h)\\right] \\\\\n",
    "    &= \\frac{h}{6}\\left( f(x_i) + 4f(x_i) + 2h f'(x_i) + \\frac{h^2}{2}f''(x_i) + \\frac{h^3}{12}f^{(3)}(x_i) + \\frac{h^4}{96}f^{(4)}(x_i) \\right.\\\\\n",
    "    & \\left. \\quad + f(x_i) + h f'(x_i) + \\frac{h^2}{2}f''(x_i) + \\frac{h^3}{6}f^{(3)}(x_i) + \\frac{h^4}{24}f^{(4)}(x_i)\n",
    "    O(h^4) \\right) \\\\\n",
    "    &= h f(x_i) + \\frac{h^2}{2}f'(x_i) + \\frac{h^3}{6}f''(x_i) + \\frac{h^4}{24}f^{(3)}(x_i) + \\frac{5h^5}{576}f^{(4)}(x_i) + O(h^6)\n",
    "\\end{align*}\n",
    "Therefore, \n",
    "\\begin{align*}\n",
    "    S(n) - I = \\sum_{i=0}^{n-1}\\left(I_i - \\int_{x_i}^{x_{i+1}}p_i(x)dx\\right) &= -\\frac{h^5}{2880}\\sum_{i=0}^{n-1}f^{(4)}(x_i) + n O(h^5) \\\\\n",
    "    &\\le \\frac{(b-a)^5}{2880n^5} n \\sup_{x\\in [a, b]}|f^{(4)}(x)| + n O\\left(\\frac{(b-a)^6}{n^6}\\right) \\\\\n",
    "    &= O(n^{-4})\n",
    "    .\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the jump of accuracy order from $O(n^{-2})$ (Trapezoidal) to $O(n^{-4})$ (Simpson)!\n",
    "\n",
    "Also note that Simpson's method use **twice** more points than the Trapezoidal method. Hence for fair comparison, compare them with the same number of evaluation points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson <- function(f, a, b, n) {\n",
    "    h <- (b - a) / n   \n",
    "    i <- seq_len(n - 1)\n",
    "    xi <- a + i * h \n",
    "    xmid <- c(xi - h / 2, b - h / 2)\n",
    "    intgrl <- h * (f(a) + 2 * sum(f(xi)) + 4 * sum(f(xmid)) + f(b)) / 6\n",
    "   \n",
    "    return(intgrl)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f <- function(x) sqrt(1-x^2)   # integrand\n",
    "simpson(f, 0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson <- function(f, a, b, n) {\n",
    "  h <- (b - a) / n\n",
    "   \n",
    "  xi <- seq.int(a, b, length.out = n + 1)\n",
    "  xi <- xi[-1]\n",
    "  xi <- xi[-length(xi)]\n",
    " \n",
    "  intgrl <- (h / 3) * (f(a) + 2 * sum(f(xi[seq.int(2, length(xi), 2)])) + \n",
    "                       4 * sum(f(xi[seq.int(1, length(xi), 2)])) + f(b))\n",
    "   \n",
    "  return(intgrl)\n",
    "   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simpson(f, 0, 1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numsub <- c(10, 100, 1000, 10000, 100000)\n",
    "err_S <- numeric(length(numsub))\n",
    "for (i in seq_along(numsub)) {\n",
    "    n <- numsub[i]\n",
    "    S <- simpson(f, 0, 1, n)\n",
    "    err_S[i] <- S - truth\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot() + geom_point(aes(x=numsub, y=abs(err_S))) + geom_line(aes(x=numsub, y=abs(err_S))) + \n",
    "scale_x_log10() + scale_y_log10() + theme_bw(base_size = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp <- data.frame(method = factor(c(rep(\"riemann\", length(numsub)), rep(\"trapezoidal\", length(numsub)), \n",
    "                                rep(\"simpson\", length(numsub)) )),\n",
    "                   numsub = c(numsub = rep(numsub, 3)), \n",
    "                   error = c(err_R, err_T, err_S) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data = comp, aes(numsub, abs(error))) + geom_point(aes(colour=method)) + geom_line(aes(colour=method)) + \n",
    "scale_x_log10() + scale_y_log10() + theme_bw(base_size = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General $m$\n",
    "\n",
    "Use the Lagrange polynomial \n",
    "$$\n",
    "L_{ij}^m(x) = \\prod_{k\\neq j}^m \\frac{x - x_{ik}^*}{x_{ij}^* - x_{ik}^*}\n",
    "$$\n",
    "for equally spaced $x_{ij}^*$ in $[x_i, x_{i+1}]$ such that\n",
    "$$\n",
    "    p_i(x) = \\sum_{j=0}^m f(x_{ij}^*)L_{ij}^m(x)\n",
    "    .\n",
    "$$\n",
    "\n",
    "It is easy to check\n",
    "$$\n",
    "L_{ij}^m(x_{ik}^*) = \\begin{cases} 1, & k = j , \\\\\n",
    "                    0, & k \\neq j.\n",
    "                  \\end{cases}\n",
    "$$\n",
    "Thus $p_i(x)$ is a polynomial of degree at most $m$ that interpolates $(x_{ij}^*, f(x_{ij}^*))$.\n",
    "\n",
    "The Lagrange polynomial proves the *interpolation theorem*:\n",
    "\n",
    "**Theorem 1**. Given $m+1$ distinct points $x_0, \\dotsc, x_m \\in \\mathbb{R}$ and the corresponding values $y_0, \\dotsc, y_m \\in \\mathbb{R}$, there exists a unique polynomial of degree at most $m$ that interpolates $\\{(x_0, y_0), \\dotsc, (x_m, y_m)\\} \\subset \\mathbb{R}^2$.\n",
    "\n",
    "*Proof* The existence of such a polynomial is shown above. To show uniqueness, let \n",
    "$$\n",
    "    p(x) = \\sum_{j=0}^m y_j L_j^m(x)\n",
    "$$\n",
    "be the interpolating polynomial constructed using the Lagrange polynomial, and $q(x)$ be another interpolating polynomial of degree at most $m$. Then we have\n",
    "\\begin{align*}\n",
    "    p(x_0) &= q(x_0) \\\\\n",
    "    &\\vdots \\\\\n",
    "    p(x_m) &= q(x_m),\n",
    "\\end{align*}\n",
    "so that $f(x) = p(x) - q(x)$ has $m+1$ distinct roots. Since $f(x)$ is a polynomial of degree at most $m$, by the fundamental theorem of algebra, it can have at most $m$ roots. Therefore, $f(x) = 0$ for all $x$ or \n",
    "$$\n",
    "    p(x) = q(x)\n",
    "$$\n",
    "for all $x$. \n",
    "\n",
    "**Corollary 1**. Let $f(x)$ is a polynomial of degree at most $m$. Suppose $x_0, \\dotsc, x_m$ subdivide $[a, b]$ in a equally spaced fashion. \n",
    "Then\n",
    "$$\n",
    "    \\int_a^b f(x)dx = \\sum_{j=0}^m w_j f(x_j),\n",
    "$$\n",
    "where $w_j = \\int_a^b L_j^m(x)dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian quadrature\n",
    "\n",
    "In Newton-C么tes quadrature, evaluation points $x_{ij}^*$ are **equally spaced** in $[x_i, x_{i+1}]$ and the integration $I_i$ is approximated as\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx \\approx \\sum_{j=0}^m w_{ij}f(x_{ij}^*)\n",
    "$$\n",
    "by choosing $w_{ij}$ optimally, in the sense that the approximation is *exact* if $f$ is up to $m$th degree polynomial (Corollary 1).\n",
    "\n",
    "We may remove the *constraint* of equal spacing and optimally choose both $w_{ij}$ and $x_{ij}^*$.\n",
    "\n",
    "**Theorem 2**. If $f$ is a polynomial of degree at most $2m+1$, then there exist $\\{x_{i0}^*, \\dotsc, x_{im}^*\\} \\subset [x_i, x_{i+1}]$ and $\\{w_{i0}, \\dotsc, w_{im}\\} \\subset \\mathbb{R}$ such that\n",
    "$$\n",
    "    I_i = \\int_{x_i}^{x_{i+1}}f(x)dx = \\sum_{j=0}^m w_{ij}f(x_{ij}^*)\n",
    "    .\n",
    "$$\n",
    "Furthermore, $w_{ij} > 0$ for all $j$.\n",
    "\n",
    "Thus the quadrature rule using $m+1$ points can be exact up to $2m+1$ degree polynomial, a big improvement from Newton-C么tes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Linear Algebra\n",
    "\n",
    "Let $\\mathcal{P}^m$ be the set of all polynomials of degree at most $m$. We all know that $\\mathcal{P}^m$ is a vector space (over the real numbers) of dimension $m+1$, and $\\{1, x, x^2, \\dotsc, x^m\\}$ forms a basis.\n",
    "\n",
    "It is easy to check for any given distinct points $\\{(x_0, y_0), \\dotsc, (x_m, y_m)\\}$, $\\{L_0^m(x), \\dotsc, L_m^m(x)\\}$ also forms a basis of $\\mathcal{P}^m$.\n",
    "\n",
    "Now consider an interval $(a, b) \\subset \\mathbb{R}$, and $\\mathcal{P}^m(a,b)$ be the $\\mathcal{P}^m$ restricted to $(a, b)$. If we define\n",
    "$$\n",
    "    \\langle \\mathbf{p}, \\mathbf{q} \\rangle = \\int_a^b p(x)q(x)dx: \\quad \\mathbb{P}^m(a,b) \\times \\mathbb{P}^m(a,b) \\to \\mathbb{R}\n",
    "$$\n",
    "for $\\mathbf{p}=p(x)$ and $\\mathbf{q}=q(x)$ in $\\mathbb{P}^m(a,b)$, then $\\langle \\mathbf{p}, \\mathbf{q} \\rangle$ is an inner product of $\\mathbf{p}$ and $\\mathbf{q}$, and $\\mathbb{P}^m(a,b)$ is an inner product space:\n",
    "1. $\\langle \\mathbf{p}, \\mathbf{p} \\rangle \\ge 0$;\n",
    "2. $\\langle \\mathbf{p}, \\mathbf{p} \\rangle = 0 \\iff \\mathbf{p} = 0$, i.e., $p(x)=0$ for all $x\\in[a, b]$;\n",
    "3. $\\langle \\alpha \\mathbf{p}, \\mathbf{q} \\rangle = \\alpha \\langle \\mathbf{p}, \\mathbf{q} \\rangle$, $\\alpha \\in \\mathbb{R}$;\n",
    "4. $\\langle \\mathbf{p}, \\mathbf{q} \\rangle = \\langle \\mathbf{q}, \\mathbf{p} \\rangle$;\n",
    "5. $\\langle \\mathbf{p} + \\mathbf{q}, \\mathbf{r} \\rangle = \\langle \\mathbf{p}, \\mathbf{r} \\rangle + \\langle \\mathbf{q}, \\mathbf{r} \\rangle$.\n",
    "\n",
    "This inner product allows us to define a norm $\\|\\mathbf{p}\\|$ over $\\mathcal{P}^m(a,b)$:\n",
    "$$\n",
    "    \\|\\mathbf{p}\\| = \\langle \\mathbf{p}, \\mathbf{p} \\rangle^{1/2} =\n",
    "    \\left(\\int_a^b p^2(x)dx\\right)^{1/2}.\n",
    "$$\n",
    "\n",
    "An orthonormal basis of $\\mathcal{P}^m(a, b)$ can be constructed by using the Gram-Schmidt procedure: let $\\mathbf{z}_j = z_j(x) = x^{j}$, $j=0, \\dotsc, m$. Then set $\\mathbf{q}_0 = \\mathbf{z}_0 / \\|\\mathbf{z}_0\\|$ and\n",
    "\\begin{align*}\n",
    "    \\tilde{\\mathbf{q}}_j &= \\mathbf{z}_j - \\sum_{k=0}^{j-1}\\langle \\mathbf{z}_j, \\mathbf{q}_k \\rangle \\mathbf{q}_k \\\\\n",
    "    \\mathbf{q}_j &= \\tilde{\\mathbf{q}}_j / \\|\\tilde{\\mathbf{q}}_j\\|\n",
    "\\end{align*}\n",
    "for $j=1, \\dotsc, m$. This amounts to\n",
    "\\begin{align*}\n",
    "    \\tilde{q}_j(x) &= x^j - \\sum_{k=0}^{j-1}\\left(\\int_a^b t^j q_k(t) dt\\right) q_k(x) \\\\\n",
    "    q_j(x) &= \\frac{\\tilde{q}_j(x)}{\\left(\\int_a^b \\tilde{q}_j^2(t)dt\\right)^{1/2}}\n",
    "\\end{align*}\n",
    "\n",
    "Orthogonal polynomial $q_j(x)$ are called the Legendre polynomial. Note that Legendre polynomials are nested:\n",
    "$$\n",
    "\\text{span}\\{\\mathbf{q}_0, \\dotsc, \\mathbf{q}_m\\}\n",
    "= \\mathcal{P}^{m}(a,b) \\subset \\mathcal{P}^{m+1}(a, b)\n",
    "= \\text{span}\\{\\mathbf{q}_0, \\dotsc, \\mathbf{q}_m, \\mathbf{q}_{m+1} \\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Theorem 2\n",
    "\n",
    "*Proof of Theorem 2*. WLOG change the notation to $\\{x_0=a, x_1, \\dotsc, x_{m-1}, x_m = b\\}$ and $\\{w_0, \\dotsc, w_m\\}$. \n",
    "\n",
    "Since $\\mathbf{f} = f(x) \\in \\mathcal{P}^{2m+1}(a, b)$, polynomial division yields\n",
    "$$\n",
    "    f(x) = g(x)q_{m+1}(x) + r(x),\n",
    "$$\n",
    "where $\\mathbf{q}_{m+1} = q_{m+1}(x)$ is the $m+1$st Legendre polynomial in $\\{\\mathbf{q}_0, \\dotsc, \\mathbf{q}_{2m}\\}$, which forms an orthonomal basis of $\\mathcal{P}^{2m+1}[a, b]$; $\\mathbf{g} = g(x)$ and $\\mathbf{r} = r(x)$ are polynomials of degree at most $m$. By the construction of the Legendre polynomials, we see that \n",
    "$$\n",
    "\\langle \\mathbf{g}, \\mathbf{q}_{m+1} \\rangle = \\int_a^b g(x)q_{m+1}(x)dx = 0\n",
    ".\n",
    "$$\n",
    "Thus $\\int_a^b f(x)dx = \\int_a^b r(x)dx$.\n",
    "\n",
    "Now choose $x_0, \\dotsc, x_m$ as the distinct real roots of the $m+1$st degree polynomial $q_{m+1}(x)$. \n",
    "(Such roots exist indeed; see Lemma 1 below.)\n",
    "Then\n",
    "$$\n",
    "    f(x_j) = q(x_j)q_{m+1}(x_j) + r(x_j) = r(x_j), \\quad j=0, \\dotsc, m.\n",
    "$$\n",
    "From Corollary 1, \n",
    "$$\n",
    "    \\int_a^b r(x) dx = \\sum_{j=0}^m w_j r(x_j), \\quad w_j = \\int_a^b L_j^m(x) dx,\n",
    "$$\n",
    "where $L_j^m(x)$ is the $j$th Lagrange polynomial given $\\{x_0, \\dotsc, x_m\\}$ and $\\{r(x_0), \\dotsc, r(x_m)\\}$.\n",
    "\n",
    "Thus\n",
    "$$\n",
    "    \\int_a^b f(x)dx = \\int_a^b r(x) dx = \\sum_{j=0}^m w_j r(x_j) = \\sum_{j=0}^m w_j f(x_j),\n",
    "$$\n",
    "as desired.\n",
    "\n",
    "It remains to show that $w_k > 0$ for any $k\\in \\{0, 1, \\dotsc, m\\}$. Set $f(x) = [L_k^m(x)]^2$, a $2m$-degree polynomial. By the definition of the Lagrange polynomial, \n",
    "$$\n",
    "    f(x_j) = \\begin{cases} 1 & j = k \\\\\n",
    "                0 & j \\neq k\n",
    "             \\end{cases}\n",
    "$$\n",
    "So,\n",
    "$$\n",
    "    0 < \\int_a^b f(x) dx = \\sum_{j=0}^m w_j f(x_j) = \\sum_{j=0}^m w_j\\delta_{jk} = w_k\n",
    "    .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lemma 1**. The roots of $k$th Legendre polynomial $q_k(x)$ are all real and distinct. \n",
    "\n",
    "*Proof*. Suppose the contrary is true. Then $q_k(x)$ changes its sign fewer than $k$ times. \n",
    "Let the roots of sign changes be $r_1 < \\dotsb < r_l$ for $l < k$.\n",
    "Then $q_k(x)\\prod_{i=1}^l (x - r_i)$ will not suffer sign changes, only being zero at $r_1, \\dotsc, r_l$. Thus\n",
    "$$\n",
    "    \\int_a^b q_k(x)\\prod_{i=1}^l (x - r_i) dx \\neq 0,\n",
    "$$\n",
    "which is a contradiction since the degree of polynomial $\\prod_{i=1}^l (x - r_i)$ is less than $k$ and is orthogonal to $q_k(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree $2m+1$ is maximal\n",
    "\n",
    "Consider $f(x) = \\prod_{j=0}^m (x-x_j)^2$, where $x_j$ are the roots of $q_{m+1}$. Obviously $f$ is a $(2m+2)$-degree polynomial. However,\n",
    "$$\n",
    "    0 < \\int_a^b f(x) \\neq \\sum_{j=0}^m w_j f(x_j)\n",
    "$$\n",
    "since $f(x_j) = 0$ for all $j = 0, \\dotsc, m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal polynomials\n",
    "\n",
    "If we change the inner product of $\\mathcal{P}^m(a, b)$ to \n",
    "$$\n",
    "    \\langle \\mathbf{p}, \\mathbf{q} \\rangle = \\int_a^b p(x)q(x)w(x) dx\n",
    "$$\n",
    "for a function $w(x)$ proportional to a density function on $(a, b)$, then the Gram-Schmidt procedure on $\\{1, x, \\dotsc, x^m\\}$ generates a different sequence of orthogonal polynomials, where the orthogonality is with respect to $w$ (called $w$-orthogonal). The Legendre polynomial corresponds to the uniform distribution on $(a, b)$.\n",
    "\n",
    "Typical choice of $w$ and associated $w$-orthogonal polynomials are tabulated:\n",
    "\n",
    "| othogonal polynomial | weight function ($w(x)$)          | domain              | distribution|\n",
    "|:---------------------|:---------------------------------:|:-------------------:|:------------|\n",
    "| Legendre             |  1                                | $(-1, 1)$            | uniform     |\n",
    "| Laguerre             | $x^{\\alpha}e^{-x}$                | $(0, \\infty)$       | gamma       |\n",
    "| Hermite              | $e^{-x^2}$                        | $(-\\infty, \\infty)$ | normal      |\n",
    "| Jacobi               | $(1-x)^{\\alpha}(1+x)^{\\beta}$ | $(-1, 1)$           | beta*       |\n",
    "\n",
    "(*) After change of variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legendre polynomial\n",
    "\n",
    "The $m$th degree Legendre polynomial (on $(-1, 1)$) is a solution to Legendre's ordinary differential equation \n",
    "$$\n",
    "    (1 - x^2)u'' - 2x u' + m(m + 1)m = 0\n",
    "$$\n",
    "given by\n",
    "$$\n",
    "    q_m(x) = \\frac{(-1)^m}{2^m m!}\\frac{d^m}{dx^m}[(1 - x^2)^m].\n",
    "$$\n",
    "\n",
    "#### Laguerre polynomial\n",
    "\n",
    "The $m$th degree Laguerre polynomial is a solution to Laguerre's ordinary differential equation \n",
    "$$\n",
    "    xu'' + (\\alpha + 1 - x)u' + m u = 0\n",
    "$$\n",
    "given by\n",
    "$$\n",
    "    q_m(x) = \\Gamma(m + \\alpha)\\sum_{k=0}^m\\binom{m}{k}\\frac{(-1)^k x^k}{\\Gamma(k + \\alpha)}.\n",
    "$$\n",
    "\n",
    "\n",
    "#### Hermite polynomial\n",
    "\n",
    "The $m$th degree Hermite polynomial is a solution to Hermites's ordinary differential equation \n",
    "$$\n",
    "    u'' - 2xu' + 2m u = 0\n",
    "$$\n",
    "given by\n",
    "$$\n",
    "    q_m(x) = (-1)^m e^{x^2}\\frac{d^m}{dx^m}e^{-x^2}.\n",
    "$$\n",
    "\n",
    "#### Jacobi polynomial\n",
    "\n",
    "The $m$th degree Jacobi polynomial is a solution to Jacobi's ordinary differential equation \n",
    "$$\n",
    "    (1 - x^2)u'' + (\\beta - \\alpha + (\\alpha + \\beta + 2)x)u' + m(m + \\alpha + \\beta + 1) u = 0\n",
    "$$\n",
    "given by\n",
    "$$\n",
    "    q_m(x) = \\frac{\\Gamma(\\alpha + m + 1)}{m!\\Gamma(\\alpha + \\beta + m + 1)}\\sum_{k=0}^m\n",
    "    \\frac{\\Gamma(\\alpha + \\beta + n + m + 1)}{m!\\Gamma(\\alpha + m + 1)}\n",
    "    \\left(\\frac{x - 1}{2}\\right)^k\n",
    "    .\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of orthogonal polynomials\n",
    "\n",
    "What we need in Gaussian quadrature is the roots of the $m$th degree orthogonal polynomial (with respect to $w$). These roots are usually found by using Newton's method. The following properties of orthogonal polynomials are useful:\n",
    "\n",
    "#### 3-term recursion\n",
    "\n",
    "$$\n",
    "    q_m(x) = (\\alpha_m + x\\beta_m)q_{m-1}(x) - \\gamma_m q_{m-2}(x)\n",
    "$$\n",
    "\n",
    "for some sequence $\\alpha_m$, $\\beta_m$, and $\\gamma_m$.\n",
    "\n",
    "\n",
    "#### Interlacing property\n",
    "\n",
    "If $\\mu_1 < \\dotsb < \\mu_m$ are the roots of $q_m(x)$, then the roots of $q_{m+1}(x)$ lie in each of the intervals\n",
    "$$\n",
    "    (-\\infty, \\mu_1), ~ (\\mu_1, \\mu_2), \\dotsc, (\\mu_m, \\infty).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gauss-Hermite quadrature\n",
    "\n",
    "If we want to integrate $f(x)$ from $-\\infty$ to $\\infty$, then Hermite polynomial is the choice:\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x)dx\n",
    "    = \\int_{-\\infty}^{\\infty} \\left(\\frac{f(x)}{e^{-x^2/2}}\\right) e^{-x^2/2}dx\n",
    "    \\approx\n",
    "    \\sum_{j=0}^m w_j f(x_j)e^{x_j^2/2}\n",
    "    ,\n",
    "$$\n",
    "where $\\{w_j\\}$ and $\\{x_j\\}$ are determined by the (scaled) Hermite polynomial.\n",
    "\n",
    "* The approximation is good if $\\hat{f}(x) = f(x)e^{x^2/2}$ is closed to a polynomial.\n",
    "\n",
    "* If $\\hat{f}$ is far from a polynomial, e.g., $f$ is concentrated on a point far from the origin, then $x_j$ will not be placed where most of the mass of $f$ is located, and GHQ may result in a poor approximation.\n",
    "    - Example: $f(x) = \\exp(-(x - \\mu)^2/(2\\sigma^2))$.\n",
    "$$\n",
    "        \\int_{-\\infty}^{\\infty} f(x) dx \n",
    "        = \\int_{-\\infty}^{\\infty} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2} + \\frac{x^2}{2\\sigma^2}\\right)\\exp\\left(-\\frac{x^2}{2}\\right)dx\n",
    "$$\n",
    "    - Then, \n",
    "$$\n",
    "    \\hat{f}(x) = \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n",
    "    \\propto \\exp\\left(\\frac{1}{2}\\left(1 - \\frac{1}{\\sigma^2}\\right)\\left(x - \\frac{\\mu}{\\sigma^2 - 1}\\right)^2 \\right).\n",
    "$$\n",
    "    - If $\\mu \\gg 0$ or $\\sigma^2 \\gg 1$, then $\\hat{f}$ will be difficult to approximate with a polynomial.\n",
    "    \n",
    "* Remedy: change of variable. Set $z = \\frac{x - \\mu}{\\sigma}$. Then,\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x) dx = \\sigma\\int_{-\\infty}^{\\infty} e^{-z^2/2}dz\n",
    "    = \\sigma w_0, \n",
    "$$\n",
    "a 1-point GHQ suffices!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statmod::gauss.quad() calculates nodes and weights for Gaussian quadrature\n",
    "statmod::gauss.quad(1, \"hermite\")   # this computes w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(pi)   # int_{-\\infty}^{\\infty}exp(-x^2)dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Binomial-logit mixture\n",
    "\n",
    "Suppose $x$ is a random sample from a Binomial distribution with $n$ trials and success probability $p=\\frac{e^{\\theta}}{1 + e^{\\theta}}$.\n",
    "Suppose the prior on $\\theta$ is $N(0, \\sigma^2)$. \n",
    "\n",
    "* Prior density: $f_{\\Theta}(\\theta) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-x^2/(2\\sigma^2)}$.\n",
    "* Likelihood: $f_{X|\\Theta}(x|\\theta) = \\binom{n}{x}p^x(1 - p)^{n-x}$, with\n",
    "$p=\\frac{e^{\\theta}}{1 + e^{\\theta}}$.\n",
    "* Posterior density:\n",
    "\\begin{align*}\n",
    "    f_{\\Theta|X}(\\theta|x) &= \\frac{f_{X|\\Theta}(x | \\theta)f_{\\Theta}(\\theta)}{\\int_{-\\infty}^{\\infty}f_{X|\\Theta}(x | \\theta')f_{\\Theta}(\\theta')d\\theta'}\n",
    "    = \\frac{p(\\theta)^x[1-p(\\theta)]^{n-x}e^{-\\theta^2/(2\\sigma^2)}}{\\int_{-\\infty}^{\\infty}p(\\theta)^x[1-p(\\theta)]^{n-x}e^{-\\theta^2/(2\\sigma^2)}d\\theta}\n",
    "    \\\\\n",
    "    &= \\frac{e^{\\theta x}[1+e^{\\theta}]^{-n}e^{-\\theta^2/(2\\sigma^2)}}{\\int_{-\\infty}^{\\infty}e^{\\theta x}[1+e^{\\theta}]^{-n}e^{-\\theta^2/(2\\sigma^2)}d\\theta}\n",
    "\\end{align*}\n",
    "\n",
    "Thus if we define $g(\\theta) = \\frac{e^{\\theta x}}{[1 + e^{\\theta}]^n}$,\n",
    "then the normalizing constant is\n",
    "$$\n",
    "\\sigma\\sqrt{2}\\int_{-\\infty}^{\\infty} g(\\sigma\\sqrt{2}y)w(y)dy\n",
    "$$ \n",
    "where $w(y) = e^{-y^2}$,\n",
    "the weight function for the Hermite polynomials. However, this turns out to be a bad idea:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g <- function(theta, x, n) exp(theta * x - n * log(1 + exp(theta)))\n",
    "\n",
    "x <- 18  # data\n",
    "n <- 20  # sample size\n",
    "sigma <- 10 # standard deviation of the prior\n",
    "\n",
    "m <- 40   # degree of Hermite polynomial\n",
    "ghq <- statmod::gauss.quad(m + 1, \"hermite\")\n",
    "denom <- sigma * sqrt(2) * sum(g(sigma * sqrt(2) * ghq$nodes, x, n) * ghq$weights)\n",
    "denom\n",
    "# R built-in adaptive quadrature\n",
    "integrate(function(y) sigma * sqrt(2) * g(sigma * sqrt(2) * y, x, n) * exp(-y^2) , -Inf, Inf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason is that the integrand $g(\\theta)$ has maximum at $p(\\theta) = x/n = 0.9$ or $\\theta = \\log\\frac{p}{1-p} = 2.197 \\gg 0$.\n",
    "\n",
    "A better apporach is the set $\\bar{g}(\\theta) = \\frac{e^{\\theta x}}{[1 + e^{\\theta}]^n}e^{-\\theta^2/(2\\sigma^2)}$ and find the maximizer $\\theta^*$ of $g$, and change the variable to $z = \\theta - \\theta^*$. Then, the normalizing constant is\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty}g(\\theta)d\\theta \n",
    "    = \\int_{-\\infty}^{\\infty} g(z + \\theta^*)dz\n",
    "    = \\int_{-\\infty}^{\\infty} g(z + \\theta^*)e^{z^2}e^{-z^2}dz\n",
    "    = \\int_{-\\infty}^{\\infty}\\tilde{g}(z)w(z)dz,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "    \\tilde{g}(z) = \\bar{g}(z + \\theta^*)e^{z^2}\n",
    "    .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat <- optimize(function(y) -g(y, x, n), c(-5, 5))$minimum  # univariate minimizer\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 <- function(z, x, n, sigma) g(z + theta_hat, x, n) * exp(-(z + theta_hat)^2 / 2 / sigma^2 + z^2)\n",
    "denom2 <- sum(g2(ghq$nodes, x, n, sigma) * ghq$weights)\n",
    "denom2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laplace approximation\n",
    "\n",
    "Laplace approximation concerns replacing the integral\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x)e^{-a g(x)} dx\n",
    "$$\n",
    "by optimization. WLOG assume that $g$ has its minimum at 0. If $g$ is three times continuously differentiable, then Taylor expansion around 0 yields\n",
    "$$\n",
    "    g(x) = g(0) + g'(0)x + \\frac{1}{2}g''(0)x^2 + O(x^3) = g(0) + \\frac{1}{2}g''(0)x^2 + o(x^2).\n",
    "$$\n",
    "Thus\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x)e^{-a g(x)} dx\n",
    "    \\approx\n",
    "    f(0)e^{-a g(0)}\\int_{-\\infty}^{\\infty}\\exp\\left(-\\frac{a g''(0)x^2}{2}\\right)dx\n",
    "    = f(0)e^{-ag(0)}\\sqrt{\\frac{2\\pi}{ag''(0)}}\n",
    "$$\n",
    "as $a \\to \\infty$, as in this case contributions to the integral is dominated by those around 0.\n",
    "This amounts to reduce the integral to integration against $N(0, 1/[ag''(0)])$. A formal statement of the Laplace approximation is given below.\n",
    "\n",
    "**Theorem 3**. If $g$ satisfies the following conditions\n",
    "1. for every $\\delta > 0$ there exists a $\\rho > 0$ such that $g(x) - g(0) \\ge \\rho$ for all $x$ with $|x| \\ge \\delta$;\n",
    "2. $g$ is twice continuously differentiable in a neighborhood of 0 and $g''(0) > 0$;\n",
    "3. $f$ is continuous in a neighborhood of 0 and $f(0) > 0$;\n",
    "4. the integral $\\int_{-\\infty}^{\\infty}f(x)e^{-ag(x)}dx$ is absolutely convergent for $a \\ge a_0$;\n",
    "\n",
    "then, \n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x)e^{-a g(x)} dx\n",
    "    \\asymp\n",
    "    f(0)e^{-ag(0)}\\sqrt{\\frac{2\\pi}{ag''(0)}}\n",
    "$$\n",
    "as $a\\to\\infty$. That is, \n",
    "$$\n",
    "    \\lim_{a\\to\\infty} \\left(\\int_{-\\infty}^{\\infty} f(x)e^{-a g(x)} dx\\right)/\\left(f(0)e^{-ag(0)}\\sqrt{\\frac{2\\pi}{ag''(0)}}\\right) = 1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "#### Stirling's formula\n",
    "\n",
    "The gamma function\n",
    "$$\n",
    "    \\Gamma(t) = \\int_0^{\\infty} x^{t - 1}e^{-x} dx\n",
    "$$\n",
    "generalizes the factorial operation: $\\Gamma(n+1) = n!$. If we define $z = x / t$, then\n",
    "$$\n",
    "    \\Gamma(t+1) = t^{t+1}\\int_0^{\\infty}e^{-t g(z)}dz\n",
    "$$\n",
    "for $g(z) = z - \\log z$. Since $g$ has its minimum at $z=1$, applying the Laplace approximation yields\n",
    "$$\n",
    "    \\Gamma(t+1) \\asymp \\sqrt{2\\pi}t^{t+1/2}e^{-t}\n",
    "$$\n",
    "as $t\\to \\infty$, which is the famous Stirling's formula for $t!$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n <- 70\n",
    "factorial(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt(2*pi) * n^(n + 0.5) * exp(-n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior expectation\n",
    "\n",
    "Recall Bayesian inference:\n",
    "\n",
    "- Parameter $\\theta$ has a prior density $f_{\\Theta}(\\theta)$ on $\\mathbb{R}$\n",
    "- Likelihood of data $x$ is $f_{X|\\Theta}(x|\\theta)$\n",
    "- Posterior density\n",
    "$$\n",
    "    f_{\\Theta|X}(\\theta|x) = \\frac{f_{X|\\Theta}(x|\\theta)f_{\\Theta}(\\theta)}{\\int_{-\\infty}^{\\infty} f_{X|\\Theta}(x|\\theta')f_{\\Theta}(\\theta')d\\theta'}\n",
    "$$\n",
    "\n",
    "We may want to evaluate posterior expectation $\\mathbf{E}[h(\\Theta) | X_1, \\dotsc, X_n]$ given $n$ independent observations. The required expectation takes the form\n",
    "$$\n",
    "    \\frac{\\int e^{\\tilde{h}(\\theta)}e^{\\ell_n(\\theta) + \\pi(\\theta)}d\\theta}{\\int e^{l_n(\\theta) + \\pi(\\theta)}d\\theta}\n",
    "$$\n",
    "where $\\tilde{h}(\\theta) = \\log h(\\theta)$, $\\ell_n(\\theta) = \\sum_{i=1}^n \\log f_{X|\\Theta}(x_i|\\theta)$ is the the log likelihood of the data, and $\\pi(\\theta) = \\log f_{\\Theta}(\\theta)$.\n",
    "\n",
    "If $n$ is large, usually the log posterior $\\ell_n(\\theta) + \\pi(\\theta)$ is sharply peaked around $\\hat{\\theta}$, the posterior mode.\n",
    "\n",
    "Using the Laplace approximation, the denominator is approximated by\n",
    "$$\n",
    "    \\int e^{\\ell_n(\\theta) + \\pi(\\theta)}d\\theta \n",
    "    \\approx e^{\\ell_n(\\hat{\\theta})+\\pi(\\hat{\\theta})} \n",
    "    \\sqrt{\\frac{2\\pi}{-[\\ell_n''(\\hat{\\theta}) + \\pi''(\\hat{\\theta})]}}\n",
    "    .\n",
    "$$\n",
    "\n",
    "If $\\tilde{h}(\\theta) + \\ell_n(\\theta) + \\pi(\\theta)$ takes its maximum at $\\tilde{\\theta}$, then the numerator is approximated by\n",
    "$$\n",
    "    \\int e^{\\tilde{h}(\\theta) + \\ell_n(\\theta) + \\pi(\\theta)}d\\theta \n",
    "    \\approx e^{\\tilde{h}(\\tilde{\\theta}) + \\ell_n(\\tilde{\\theta})+\\pi(\\tilde{\\theta})} \n",
    "    \\sqrt{\\frac{2\\pi}{-[\\tilde{h}(\\tilde{\\theta}) + \\ell_n''(\\tilde{\\theta}) + \\pi''(\\tilde{\\theta})]}}\n",
    "    .\n",
    "$$\n",
    "\n",
    "Thus the expectation is approximated by\n",
    "$$\n",
    "    \\exp\\left(\\tilde{h}(\\tilde{\\theta}) + \\ell_n(\\tilde{\\theta}) + \\pi(\\tilde{\\theta}) \n",
    "    - \\ell_n(\\hat{\\theta}) - \\pi(\\hat{\\theta})\\right)\n",
    "    \\sqrt{\\frac{\\ell_n''(\\hat{\\theta}) + \\pi''(\\hat{\\theta})}{\\tilde{h}''(\\tilde{\\theta}) + \\ell_n''(\\tilde{\\theta}) + \\pi''(\\tilde{\\theta})}}    \n",
    "    .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: binomial-logit mixture\n",
    "\n",
    "Recall from the binomial-logit mixture example above, set \n",
    "$$\n",
    "f(\\theta) = e^{-\\theta^2/(2\\sigma^2)},\n",
    "\\quad\n",
    "g(\\theta) = -\\theta x + n\\log(1 + e^{\\theta})\n",
    ".\n",
    "$$\n",
    "\n",
    "Then the Laplace approximation of the normalization constant of the posterior density is\n",
    "$$\n",
    "f(\\theta^*)e^{-g(\\theta^*)}\\sqrt{\\frac{2\\pi}{g''(\\theta^*)}}\n",
    "$$\n",
    "where $\\theta^* = \\log\\frac{x/n}{1 - x/n}$. It is easy to see that\n",
    "$$\n",
    "    g''(\\theta^*) = n \\frac{e^{\\theta^*}}{(1 + e^{\\theta^*})^2} = n p (1 - p),\n",
    "    \\quad\n",
    "    p = x / n.\n",
    "$$\n",
    "For the numerical example of $x=18$ and $n=20$, we have the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x <- 18; n <- 20\n",
    "p <- x / n\n",
    "theta_star <-log(p / (1-p))\n",
    "lap <- exp(-theta_star^2 / 2/ sigma^2) * exp(theta_star * x) / \n",
    "        (1 + exp(theta_star))^n * sqrt(2 * pi / n / p / (1 - p))\n",
    "lap   # Laplace approximation\n",
    "denom2  # Gauss-Hermite quadrature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: moments of beta distribution\n",
    "\n",
    "Recall that the beta distribution has density \n",
    "$$\n",
    "f_{\\Theta}(\\theta) \n",
    "= \\frac{\\Gamma(\\alpha+\\beta)\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{\\Gamma(\\alpha)\\Gamma(\\beta)} \n",
    "\\propto \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}\n",
    "$$ \n",
    "on $(0, 1)$. If the likelihood of the data $X$ is $\\text{Binom}(n, \\theta)$, then the posterior density of the success probability $\\theta$ is also beta: $f_{\\Theta|X}(\\theta|x) \\propto \\theta^{\\alpha + x -1}(1-\\theta)^{\\beta + n - x -1}$. Thus, finding the moment of a bete distribution amounts to computing the posterior moment of the success probability.\n",
    "\n",
    "The exact value of $\\mathbf{E}[\\Theta^k]$ is\n",
    "$$\n",
    "    \\mathbf{E}[\\Theta^k] = \\frac{\\int_0^1 \\theta^{k+\\alpha-1}(1-\\alpha)^{\\beta-1}d\\theta}{\\int_0^1\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}d\\theta}\n",
    "    = \\frac{\\Gamma(k + \\alpha)\\Gamma(\\beta)}{\\Gamma(k + \\alpha + \\beta)}\n",
    "    \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\n",
    "    = \\frac{\\Gamma(k + \\alpha)/\\Gamma(\\alpha)}{\\Gamma(k + \\alpha + \\beta)/\\Gamma(\\alpha + \\beta)}\n",
    "    = \\frac{(k+\\alpha -1)\\dotsb \\alpha}{(k+\\alpha+\\beta-1)\\dotsb(\\alpha+\\beta)}\n",
    "$$\n",
    "\n",
    "In order to apply the Laplace approximation, observe that $\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} = -[(\\alpha-1)\\log\\theta + (\\beta-1)\\log(1-\\theta)] =: g(\\theta)$, which has the unique minimum at $\\hat{\\theta} = \\frac{\\alpha-1}{\\alpha+\\beta-2}\\in(0, 1)$. Therefore,\n",
    "$$\n",
    "    \\int_0^1 \\theta^{\\alpha-1}(1-\\theta)^{\\beta-1} d\\theta\n",
    "    \\approx e^{-g(\\hat{\\theta})}\\sqrt{\\frac{2\\pi}{g''(\\hat{\\theta})}}\n",
    "    = \\left(\\frac{\\alpha-1}{\\alpha+\\beta-2}\\right)^{\\alpha-1}\\left(\\frac{\\beta-1}{\\alpha+\\beta-2}\\right)^{\\beta-1}\n",
    "    \\sqrt{2\\pi}\\sqrt{\\frac{(\\alpha-1)(\\beta-1)}{(\\alpha+\\beta-2)^3}}\n",
    "$$\n",
    "since $g(\\hat{\\theta}) = -\\log\\left[\\left(\\frac{\\alpha-1}{\\alpha+\\beta-2}\\right)^{\\alpha-1}\\left(\\frac{\\beta-1}{\\alpha+\\beta-2}\\right)^{\\beta-1}\\right]$ and $g''(\\hat{\\theta}) = (\\alpha-1)\\left(\\frac{\\alpha+\\beta-2}{\\alpha-1}\\right)^2 + (\\beta-1)\\left(\\frac{\\alpha+\\beta-2}{\\beta-1}\\right)^2$.\n",
    "\n",
    "Likewise,\n",
    "$$\n",
    "    \\int_0^1 \\theta^{k+\\alpha-1}(1-\\theta)^{\\beta-1} d\\theta\n",
    "    \\approx \n",
    "    \\left(\\frac{k+\\alpha-1}{k+\\alpha+\\beta-2}\\right)^{k+\\alpha-1}\\left(\\frac{\\beta-1}{k+\\alpha+\\beta-2}\\right)^{\\beta-1}\n",
    "    \\sqrt{2\\pi}\\sqrt{\\frac{(k+\\alpha-1)(\\beta-1)}{(k+\\alpha+\\beta-2)^3}}\n",
    "$$\n",
    "\n",
    "Therefore,\n",
    "$$\n",
    "    \\mathbf{E}[\\Theta^k] \\approx\n",
    "    \\left(\\frac{(k+\\alpha-2)(\\alpha+\\beta-2)^3}{(\\alpha-1)(k+\\alpha+\\beta-2)^3}\\right)^{1/2}\n",
    "    \\cdot\n",
    "    \\frac{\\left(\\frac{k+\\alpha-1}{k+\\alpha+\\beta-2}\\right)^{k+\\alpha-1}\\left(\\frac{\\beta-1}{k+\\alpha+\\beta-2}\\right)^{\\beta-1}}{\\left(\\frac{\\alpha-1}{\\alpha+\\beta-2}\\right)^{\\alpha-1}\\left(\\frac{\\beta-1}{\\alpha+\\beta-2}\\right)^{\\beta-1}}\n",
    "$$\n",
    "\n",
    "It can be shown that $\\alpha, \\beta \\to \\infty$ in such a way that $\\frac{\\alpha}{\\alpha+\\beta} \\to p \\in (0,1)$, then\n",
    "the approximation of $\\mathbf{E}[\\Theta^k]$ tends to $p^k$. Also note that\n",
    "$$\n",
    "    \\frac{(k+\\alpha -1)\\dotsb \\alpha}{(k+\\alpha+\\beta-1)\\dotsb(\\alpha+\\beta)} \\to p^k\n",
    "$$\n",
    "in this setting. Thus the Laplace approximation of the beta moment is correct in the limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multidimensional integration\n",
    "\n",
    "Laplace approximation naturally extends to multidimensional integration. On the contrary, quadrature methods needs  number of points that grows exponentially with dimension.\n",
    "\n",
    "We will see later that Monte Carlo methods can also alleviate this difficulties.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proof of Theorem 3\n",
    "\n",
    "Assume WLOG $g(0) = 0$ (multiply $e^{ag(0)}$ on both sides and set $g(x) := g(x) - g(0)$). \n",
    "Now since\n",
    "$$\n",
    "    g'(x) = g'(0) + g''(0)x + o(x) = g''(0)x + o(x)\n",
    "$$\n",
    "as $x \\to 0$, applying l'H么pital's rule yields $g(x) - \\frac{1}{2}g''(0)x^2 = o(x^2)$ as $x\\to 0$. Combining with condition 3, for a given small $\\epsilon \\in (0, \\frac{1}{2}g''(0))$ there exists $\\delta > 0$ such that\n",
    "\\begin{align*}\n",
    "    (1-\\epsilon) f(0) &\\le f(x) \\le (1+\\epsilon) f(0)  \\\\\n",
    "    -\\epsilon x^2 &\\le g(x) - \\frac{1}{2}g''(0)x^2 \\le \\epsilon x^2\n",
    "\\end{align*}\n",
    "for all $x$ with $|x| < \\delta$.\n",
    "\n",
    "Now examine the contributions to the integral from the region $|x| \\ge \\delta$. From condition 1, $g(x) \\ge \\rho$. Then for $a \\ge a_0$,\n",
    "\\begin{align*}\n",
    "    \\left|\\int_{\\delta}^{\\infty}f(x)e^{-ag(x)}dx\\right|\n",
    "    &\\le \\int_{\\delta}^{\\infty}|f(x)|e^{-(a - a_0)g(x)}e^{-a_0 g(x)}dx \\\\\n",
    "    &\\le e^{-(a - a_0)\\rho}\\int_{\\delta}^{\\infty}|f(x)|e^{-a_0 g(x)}dx \\\\\n",
    "    &\\le e^{-(a - a_0)\\rho}\\int_{-\\infty}^{\\infty}|f(x)|e^{-a_0 g(x)}dx \n",
    "    = O(e^{-\\rho a})\n",
    "    .\n",
    "\\end{align*}\n",
    "The last equality is due to condition 4. By the same argument, we also have \n",
    "$$\n",
    "\\int_{-\\infty}^{-\\delta}f(x)e^{-ag(x)}dx = O(e^{-\\rho a})\n",
    ".\n",
    "$$\n",
    "\n",
    "The central portion of the integral can be bounded above:\n",
    "\\begin{align*}\n",
    "    \\int_{-\\delta}^{\\delta} f(x)e^{-ag(x)}dx \n",
    "    & \\le (1+\\epsilon)f(0)\\int_{-\\delta}^{\\delta} e^{-\\frac{a}{2}[g''(0)-2\\epsilon]x^2} dx \\\\\n",
    "    &= (1+\\epsilon)f(0)\\int_{-\\infty}^{\\infty} e^{-\\frac{a}{2}[g''(0) - 2\\epsilon]x^2} dx\\\\\n",
    "    &= (1+\\epsilon)f(0)\\sqrt{\\frac{2\\pi}{a[g''(0)- 2\\epsilon]}}\n",
    "    .\n",
    "\\end{align*}\n",
    "Therefore,\n",
    "$$\n",
    "    \\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx \\le (1+\\epsilon)f(0)\\sqrt{\\frac{2\\pi}{a[g''(0)- 2\\epsilon]}} + O(e^{-\\rho a}),\n",
    "$$\n",
    "yielding\n",
    "$$\n",
    "    \\limsup_{a\\to\\infty} \\sqrt{a}\\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx\n",
    "    \\le (1+\\epsilon) f(0) \\sqrt{\\frac{2\\pi}{g''(0)- 2\\epsilon}}\n",
    "    .\n",
    "$$\n",
    "Sending $\\epsilon$ to zero, we have\n",
    "$$\n",
    "    \\limsup_{a\\to\\infty} \\sqrt{a}\\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx\n",
    "    \\le f(0) \\sqrt{\\frac{2\\pi}{g''(0)}}\n",
    "    .\n",
    "$$\n",
    "\n",
    "\n",
    "We can also find a lower bound of the central portion of the integral:\n",
    "\\begin{align*}\n",
    "    \\int_{-\\delta}^{\\delta} f(x)e^{-ag(x)}dx \n",
    "    & \\ge (1-\\epsilon)f(0)\\int_{-\\delta}^{\\delta} e^{-\\frac{a}{2}[g''(0)+2\\epsilon]x^2} dx \n",
    "    .\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n",
    "For $|x| \\ge \\delta$, $(\\frac{1}{2}g''(0) + \\epsilon)x^2 \\ge (\\frac{1}{2}g''(0) + \\epsilon)\\delta^2 =: \\lambda$. \n",
    "Furthermore, $\\int_{-\\infty}^{\\infty} e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx < \\infty$ for any $a > 0$.\n",
    "Hence by repeating the same argument leading to $\\int_{\\delta}^{\\infty}f(x)e^{-ag(x)}dx = O(e^{-\\rho a})$, we have\n",
    "$$\n",
    "    \\int_{-\\infty}^{-\\delta}e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2}dx\n",
    "    + \\int_{\\delta}^{\\infty}e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2}dx\n",
    "    = O(e^{-\\lambda a}).\n",
    "$$\n",
    "Therefore,\n",
    "\\begin{align*}\n",
    "    (1-\\epsilon)f(0)\\int_{-\\delta}^{\\delta} e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx \n",
    "    &= (1-\\epsilon)f(0)\\int_{-\\infty}^{\\infty} e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx \\\\\n",
    "    & \\quad - (1-\\epsilon)f(0)\\left[\\int_{-\\infty}^{-\\delta}e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx \n",
    "    + \\int_{\\delta}^{\\infty}e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx \\right]\n",
    "    \\\\\n",
    "    &\\ge (1-\\epsilon)f(0)\\int_{-\\infty}^{\\infty} e^{-\\frac{a}{2}[g''(0) + 2\\epsilon]x^2} dx - O(e^{-\\lambda a}) \\\\\n",
    "    &= (1-\\epsilon)f(0)\\sqrt{\\frac{2\\pi}{a[g''(0)+ 2\\epsilon]}} + O(e^{-\\lambda a})\n",
    "    ,\n",
    "\\end{align*}\n",
    "yielding\n",
    "$$\n",
    "    \\liminf_{a\\to\\infty} \\sqrt{a}\\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx\n",
    "    \\ge (1-\\epsilon) f(0) \\sqrt{\\frac{2\\pi}{g''(0)+ 2\\epsilon}}\n",
    "    .\n",
    "$$\n",
    "Sending $\\epsilon$ to zero, we have\n",
    "$$\n",
    "    \\liminf_{a\\to\\infty} \\sqrt{a}\\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx\n",
    "    \\ge f(0) \\sqrt{\\frac{2\\pi}{g''(0)}}\n",
    "    .\n",
    "$$\n",
    "\n",
    "\n",
    "Thus\n",
    "$$\n",
    "    \\lim_{a\\to\\infty} \\sqrt{a}\\int_{-\\infty}^{\\infty} f(x)e^{-ag(x)}dx\n",
    "    = f(0) \\sqrt{\\frac{2\\pi}{g''(0)}}\n",
    "    .\n",
    "$$\n",
    "as desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_position": {
    "height": "441.3333435058594px",
    "left": "0px",
    "right": "903.3333129882813px",
    "top": "140.6666717529297px",
    "width": "166px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
